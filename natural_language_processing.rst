======================
正規表現と自然言語処理
======================

.. contents:: :depth: 3

.. seealso::

   :title-reference:`Introduction to Information Retrieval`

      chapter 2
      chapter 3
      chapter 6

動機付け
========

情報検索は、集めたオブジェクトから特徴量を抽出し、得られた特徴から検索用のインデックスを作成します。
例えば今回の作成しているWeb検索システムの場合、クローラが集めたWebページが検索対象オブジェクトとなり、それぞれのページに記述されているテキストや埋めこまれている画像、メタデータなどがコンテンツとなります。
そして、それらのコンテンツから、時間表現や単語頻度、画像ヒストグラムなどを特徴量として抽出できます。

このように、あるオブジェクトから特徴を抽出し、それをベクトルとして表現したものを、このオブジェクトの **特徴ベクトル** と呼びます。
特徴ベクトルを用いた検索については、「文書の検索」の回で改めて解説します。

今回扱う正規表現と自然言語処理は、テキストコンテンツから特徴量を抽出するのに用いることができます。
例えば次のような文書があったとしましょう。::

  Python(パイソン)は、オランダ人のグイド・ヴァンロッサムが作ったオープンソースのプログラミング言語。
  
  最新リリース
  3.2 / 2011年2月20日

このような文書から

   時間表現の抽出
       2011年2月20日

   単語(名詞)の抽出
       Python、パイソン、オランダ人、グイド・ヴァンロッサム、オープンソース、プログラミング、言語

のような特徴を抽出できるようにすることが、今回の目標です。

正規表現
========

正規表現を理解するのに良いWebアプリ
-----------------------------------

#. RegExr

   正規表現は実際に記述しなければ理解することが難しいので、自分で試してみることが重要でが、毎回毎回Pythonのreモジュールを使って試すのは大変ですし、時間もかかります。
   
   RegExrは入力した正規表現の適合箇所を簡単に確認することができるWebアプリケーションです。
   上のテキストボックスに正規表現を入力すると、下の文書の中でそれにマッチする箇所を表示してくれるので、トライアンドエラーのサイクルが短くすることができます。
   
   プログラムを書く前に、意図した通りに表現できているかを確かめる用途にも使うことができます。
   
   URL: http://www.gskinner.com/RegExr/
   
   .. image:: /images/RegExr.png

#. strfriend

   正規表現は理論的にはオートマトンを用いて説明することができます。
   
   strfriendは入力された正規表現を表す非決定性オートマトンを出力してくれるWebアプリケーションです。
   これを用いて正規表現を可視化することで、複雑で難しい正規表現が理解しやすくなるかも知れません。
   
   URL: http://www.strfriend.com/
   
   .. image:: /images/strfriend1.png
   
   メールアドレスにマッチする正規表現を入力した場合
   
   .. image:: /images/strfriend2.png

Pythonでの使用法
----------------

.. seealso::

   Python公式ドキュメント
      `7.2. re - 正規表現操作 <http://www.python.jp/doc/nightly/library/re.html>`_

自然言語処理
============

概要
----

形態素解析・正規化
------------------

Pythonでの使用法
----------------

課題
====

課題1 ex1_re.py
---------------

1. 与えられた文字列から **時間表現を抽出** する関数(ex11)を作成せよ。

   この課題での時間表現とは *時分秒* を表し、次の形式のいずれかとする。

   A. 1:12:13
      時分秒は:で区切られる 1時12分13秒
   B. 01:12:13
      0による桁あわせ
   C. 01:12:13 pm
      12時間表記 半角スペース1個の後にpmもしくはam
   D. 01:12:13 p.m.
      12時間表記 半角スペース1個の後にp.m.もしくはa.m.
   
   **注意点**
   
   * 0時0分0秒から23時59分59秒の間のみ抽出する
     99:99:99のような表現は抽出しない
   * 14:00:00 p.m. のような表現は抽出しない
   * HWaddr 00:23:54:91:03:09 のような表現は抽出しない
   * すべてを正規表現で行う必要はない
     正規表現で時間表現の候補を抽出 -> 無効な表現を削除

2. 与えられた文字列から時間表現を抽出し、それらを **hh:mm:ss形式に正規化** する関数(ex12)を作成せよ。

   A. 1:12:13       -> 01:12:13
   B. 01:12:13 p.m. -> 13:12:13

次のコードをex1_re.pyという名前で保存し、テストが通るように実装する::

   # -*- coding: utf-8 -*-
   
   
   def ex11(text):
       '''課題1-1
       引数の文字列(text)から時間表現を抽出する。
   
           >>> ex11('1:2:3 to 1:3:3')
           ['01:02:03', '01:03:03']
           >>> ex11('updated at 0:00:00')
           ['0:00:00']
           >>> ex11('11:15:30 pm')
           ['11:15:30 pm']
           >>> ex11('11:15:30 am')
           ['11:15:30 am']
           >>> ex11('11:15:30 p.m.')
           ['11:15:30 p.m.']
           >>> ex11('11:15:30 a.m.')
           ['11:15:30 a.m.']
           >>> ex11('12:23:34 pmi conference ...')
           ['12:23:34']

       Macアドレスなどに反応してはいけない。

           >>> ex11('2011:05:17')
           []
           >>> ex11('HWaddr 00:23:54:91:03:05')
           []
           >>> ex11('23:11: ')
           []
           >>> ex11('12:234:56')
           []
           >>> ex11('14:00:00 pm')
           []
           >>> ex11('24:00:00')
           []
           >>> ex11('99:99:99')
           []
       '''
       pass
   
   
   if __name__ == '__main__':
       import doctest
       doctest.testmod()

テストは次のようにすることで実行できる::

   $ python ex1_re.py

課題2 ex2_nlp.py
----------------

1. 与えられた単語が **ストップワードであるかどうかを判別** する関数(ex21)を作成せよ。

   * 何がストップワードであるかは好きに決めていい
   * SlothLibのストップワードリストを使用してもいい
   * nltkのストップワードリスト(英語のみ利用可能)を使用してもいい

2. 与えられた文字列（日本語ベース）を **形態素解析し、名詞のみを抽出し、正規化し、ストップワードを除去した後、単語の出現回数をカウントしたディクショナリ** を作成する関数(ex22)を作成せよ。

      例えば::

         Database (<複> databases)とは、特定のテーマに沿ったデータを集めて管理し、
         容易に検索・抽出などの再利用をできるようにしたもの。

      という文字列が入力された場合::

         {"複": 1, "データ": 1, "管理": 1, "再": 1, "抽出": 1, "database": 2,
          "特定": 1, "検索": 1, "テーマ": 1, "容易": 1, "利用": 1}

次のコードをex2_nlp.pyという名前で保存し、テストが通るように実装する。::

   # -*- coding: utf-8 -*-
   
   
   def ex21(word):
       '''課題2-1
       引数の文字列(word)がストップワードであればTrueを返す
   
           >>> ex21("こと")
           True
           >>> ex21("データベース")
           False
           >>> ex21("the")
           True
           >>> ex21("database")
           False
       '''
       pass
   
   
   def ex22(text):
       '''課題2-2
       引数の文字列(text)から名詞を抽出し、正規化、 ストップワードを除去する。
       その後、単語の出現頻度をカウントしたディクショナリを返す。
       下記はあくまでも一例
   
           >>> text = """Database (<複> databases)とは、
           ... 特定のテーマに沿ったデータを集めて管理し、
           ... 容易に検索・抽出などの再利用をできるようにしたもの。"""
           >>> tf = ex22(text)
           >>> for key in sorted(tf.keys()):
           ...     print key, tf[key]
           ...
           database 2
           テーマ 1
           データ 1
           再 1
           利用 1
           容易 1
           抽出 1
           検索 1
           特定 1
           管理 1
           複 1

       ここで得られた辞書型オブジェクトtfのように、ベクトルの各次元が単語の文書中での
       出現回数となっているものをterm frequencyベクトルという。
       多くの場合、省略して単にtfベクトルとも呼ばれる。
       '''
       pass
   
   if __name__ == '__main__':
       import doctest
       doctest.testmod()

テストは次のようにすることで実行できる::

   $ python ex2_nlp.py
